{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2026ebf7-0a4d-4796-acb6-dbd818555811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c2b001-a608-4b25-9de0-04dc9ebf215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Poorva\n",
      "[nltk_data]     Marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Poorva\n",
      "[nltk_data]     Marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Poorva\n",
      "[nltk_data]     Marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Poorva Marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df4262e-65a4-488a-a960-432a6451687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph could contain a series of brief examples or a single long illustration of a general point.', 'It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "text=\"A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.\"\n",
    "tokens=nltk.sent_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d091599f-fabb-407e-b018-f2fa5a4ca960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'paragraph', 'could', 'contain', 'a', 'series', 'of', 'brief', 'examples', 'or', 'a', 'single', 'long', 'illustration', 'of', 'a', 'general', 'point', '.', 'It', 'might', 'describe', 'a', 'place', ',', 'character', ',', 'or', 'process', ';', 'narrate', 'a', 'series', 'of', 'events', ';', 'compare', 'or', 'contrast', 'two', 'or', 'more', 'things', ';', 'classify', 'items', 'into', 'categories', ';', 'or', 'describe', 'causes', 'and', 'effects', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenw=nltk.word_tokenize(text)\n",
    "print(tokenw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c90580-0444-4703-99e6-563663c35290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a390db55-e7e2-485e-b15e-17043d00a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'paragraph', 'could', 'contain', 'a', 'seri', 'of', 'brief', 'exampl', 'or', 'a', 'singl', 'long', 'illustr', 'of', 'a', 'gener', 'point', '.', 'it', 'might', 'describ', 'a', 'place', ',', 'charact', ',', 'or', 'process', ';', 'narrat', 'a', 'seri', 'of', 'event', ';', 'compar', 'or', 'contrast', 'two', 'or', 'more', 'thing', ';', 'classifi', 'item', 'into', 'categori', ';', 'or', 'describ', 'caus', 'and', 'effect', '.']\n"
     ]
    }
   ],
   "source": [
    "stem=[]\n",
    "for i in tokenw:\n",
    "    ps=PorterStemmer()\n",
    "    stem_word=ps.stem(i)\n",
    "    stem.append(stem_word)\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db6dd1d-c178-4c85-953d-581d64e9356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a paragraph could contain a seri of brief exampl or a singl long illustr of a gener point . it might describ a place , charact , or process ; narrat a seri of event ; compar or contrast two or more thing ; classifi item into categori ; or describ caus and effect .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12cad026-203e-4c69-819f-531728ea3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'paragraph', 'could', 'contain', 'a', 'seri', 'of', 'brief', 'exampl', 'or', 'a', 'singl', 'long', 'illustr', 'of', 'a', 'gener', 'point', '.', 'it', 'might', 'describ', 'a', 'place', ',', 'charact', ',', 'or', 'process', ';', 'narrat', 'a', 'seri', 'of', 'event', ';', 'compar', 'or', 'contrast', 'two', 'or', 'more', 'thing', ';', 'classifi', 'item', 'into', 'categori', ';', 'or', 'describ', 'caus', 'and', 'effect', '.']\n"
     ]
    }
   ],
   "source": [
    "leme=[]\n",
    "for i in stem:\n",
    "    lemmatized_word=lemmatizer.lemmatize(i)\n",
    "    leme.append(lemmatized_word)\n",
    "print(leme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3cc34c-63a0-4ffd-8938-4686c8913644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging: [('a', 'DT'), ('paragraph', 'NN'), ('could', 'MD'), ('contain', 'VB'), ('a', 'DT'), ('seri', 'NN'), ('of', 'IN'), ('brief', 'JJ'), ('exampl', 'NN'), ('or', 'CC'), ('a', 'DT'), ('singl', 'NN'), ('long', 'JJ'), ('illustr', 'NN'), ('of', 'IN'), ('a', 'DT'), ('gener', 'NN'), ('point', 'NN'), ('.', '.'), ('it', 'PRP'), ('might', 'MD'), ('describ', 'VB'), ('a', 'DT'), ('place', 'NN'), (',', ','), ('charact', 'NN'), (',', ','), ('or', 'CC'), ('process', 'NN'), (';', ':'), ('narrat', 'CC'), ('a', 'DT'), ('seri', 'NN'), ('of', 'IN'), ('event', 'NN'), (';', ':'), ('compar', 'NN'), ('or', 'CC'), ('contrast', 'NN'), ('two', 'CD'), ('or', 'CC'), ('more', 'JJR'), ('thing', 'NN'), (';', ':'), ('classifi', 'JJ'), ('item', 'NN'), ('into', 'IN'), ('categori', 'NN'), (';', ':'), ('or', 'CC'), ('describ', 'VB'), ('caus', 'NN'), ('and', 'CC'), ('effect', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tagging:\",nltk.pos_tag(leme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fecb9ac-47f2-4ed9-8153-f4441f022e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw=stopwords.words('english')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40901dc-ba47-4f40-a8c4-530318f7acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF matrix:\n",
      "   among  approach  geographically  gina  globally  ideas  increase  \\\n",
      "0      1         1               1     2         1      1         1   \n",
      "\n",
      "   knowledge  may  means  members  provide  separated  share  sharing  team  \\\n",
      "0          1    1      1        1        1          1      1        1     1   \n",
      "\n",
      "   thought  would  \n",
      "0        1      1  \n",
      "\n",
      "TF-IDF matrix:\n",
      "      among  approach  geographically      gina  globally     ideas  increase  \\\n",
      "0  0.218218  0.218218        0.218218  0.436436  0.218218  0.218218  0.218218   \n",
      "\n",
      "   knowledge       may     means   members   provide  separated     share  \\\n",
      "0   0.218218  0.218218  0.218218  0.218218  0.218218   0.218218  0.218218   \n",
      "\n",
      "    sharing      team   thought     would  \n",
      "0  0.218218  0.218218  0.218218  0.218218  \n",
      "\n",
      "TF-IDF score for the document:\n",
      "among: 0.22\n",
      "approach: 0.22\n",
      "geographically: 0.22\n",
      "gina: 0.44\n",
      "globally: 0.22\n",
      "ideas: 0.22\n",
      "increase: 0.22\n",
      "knowledge: 0.22\n",
      "may: 0.22\n",
      "means: 0.22\n",
      "members: 0.22\n",
      "provide: 0.22\n",
      "separated: 0.22\n",
      "share: 0.22\n",
      "sharing: 0.22\n",
      "team: 0.22\n",
      "thought: 0.22\n",
      "would: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Poorva\n",
      "[nltk_data]     Marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the document\n",
    "document = 'GINA team thought its approach would provide a means to share ideas globally and increase knowledge sharing among GINA members who may be separated geographically.'\n",
    "\n",
    "# Tokenize the document\n",
    "tokens = nltk.word_tokenize(document)\n",
    "\n",
    "# Remove stop words and convert to lowercase\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Convert the list of tokens back to a single string\n",
    "filtered_document = ' '.join(filtered_tokens)\n",
    "\n",
    "# Calculate TF (Term Frequency)\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_matrix = count_vectorizer.fit_transform([filtered_document])\n",
    "\n",
    "# Calculate IDF (Inverse Document Frequency)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame for TF matrix\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Create DataFrame for TF-IDF matrix\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Display TF matrix\n",
    "print(\"TF matrix:\")\n",
    "print(tf_df)\n",
    "\n",
    "# Display TF-IDF matrix\n",
    "print(\"\\nTF-IDF matrix:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Calculate TF-IDF score for the document\n",
    "print(\"\\nTF-IDF score for the document:\")\n",
    "tfidf_scores = zip(feature_names, tfidf_matrix.toarray()[0])\n",
    "for word, score in tfidf_scores:\n",
    "    print(f\"{word}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55585f-beeb-400f-809b-6a1ff68cbee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
